---
title: "Assignment #2: Meta-analysis of Ocean Acidification Effects on Behaviour"
output: html_document
date: "2022-10-20"
---

## UNI ID: u7141409

# i) Statistical Analysis and Interpretation
### Downloading packages
```{r}
library(pacman)
devtools::install_github("daniel1noble/orchaRd", force = TRUE) # Install the orchaRd package
pacman::p_load(readxl, tidyverse, dplyr, Rcpp, ggforce, flextable, metafor, readr, orchaRd)
```
### Task 1: generating summary statistic for each species average treatment activity
```{r}
# Importing the data as well as signing it to a variable 
getwd() # to check pathway
OA_data <- read_csv("C:/ANU study/Year 3 2022/BIOL3207/Assignment2/data/Meta-analysis_OA/OA_activitydat_20190302_BIOL3207.csv")

# Cleaning up the data 
glimpse(OA_data)
list(unique(OA_data$species)) # To check spelling errors in species
list(unique(OA_data$treatment)) # To check spelling errors in treatment

# Omitting NA in the data as they are not required for the meta-analysis 
OA <- (na.omit(OA_data))

# The n for each species 
table(OA$species)
# The n for treatments 
table(OA$treatment)

# Creating the necessary summary statistics and using flextable() function to tidy the table
OA_sum <- (OA %>%
  group_by(species, treatment) %>% 
  summarise(mean = mean(activity, na.rm = TRUE), sd = sd(activity, na.rm = TRUE),
  n = length(unique(animal_id))) %>% rename(Species = "species"))

OA_table <- flextable(OA_sum)
```
### Task 2: Merge table from task 1 with the metadata
```{r}
# This file contains the metadata and was imported
clark <- read_csv("C:/ANU study/Year 3 2022/BIOL3207/Assignment2/data/Meta-analysis_OA/clark_paper_data.csv") 

# this function is used to get two data frames and then placed in along together
metadata1 <- cbind(clark, OA_sum) 
metadata1

# Making a final metadata  for each treatment as it was instructed in task 1. 
final_meta <- pivot_wider(metadata1, names_from = treatment, names_glue = "{treatment}_{.value}", values_from = c("mean", "sd", "n"))  ### Look for a different method.. 

final_meta
```

### Task 3: Merge the output from 2 into larger meta-analysis dataset
```{r}
# This large meta-analsis dataset was imported
ocean_meta <- read_csv("C:/ANU study/Year 3 2022/BIOL3207/Assignment2/data/Meta-analysis_OA/ocean_meta_data.csv")

# This was coded to quickly observe whether the colnames were the same/ matched
dim(ocean_meta)
dim(final_meta)

## Do some renaming of colnames so they match "meta" data variable
meta <- final_meta %>% rename("oa.sd" = CO2_sd, "oa.mean" = CO2_mean, "oa.n" = CO2_n,"ctrl.sd" = control_sd,"ctrl.n" = control_n, "ctrl.mean" = control_mean)
                            
# Reorder col names based on names in meta_data_full
meta <- meta[names(ocean_meta)]

# Check columns are in same order
colnames(ocean_meta) == colnames(meta)

# Bind the two dataframes
full_final <- rbind(ocean_meta, meta)
```

### Task 4: Calculate lnRR effect size for every row of the dataframe
```{r}
full_final[1,] # to check how a row looks like

full_final <- metafor::escalc(n1i = oa.n, n2i = ctrl.n, m1i = ctrl.mean, m2i = oa.mean, sd1i = oa.sd, sd2i = ctrl.sd, data = full_final, measure = "ROM")
# yi: observed effect sizes
# vi: sampling variances

# calculating the effect size by one hand of one row to check if i got the correct calculation
13.2730^2 #176.1725
72.7890^2 #5298.239
2.9931^2 #8.958648
94.1290^2 #8860.269

(176.1725/ (5298.239*46)) + (8.958648/(8860.269*26)) #0.0007617395

# Seems like i did it wrong so retrying the escalc() function calculation 
full_final <- escalc(n1i = ctrl.n, n2i = oa.n, m1i = ctrl.mean, m2i = oa.mean, sd1i = ctrl.sd, sd2i = oa.sd, data = full_final, measure = "ROM")
# Found that I had to keep the number, mean and sd of the ctrl in the same '2' and oa in '1'
```

### Task 5: Meta-analytic model for sampling variance of lnRR - including random effect of study and obervation
```{r}
# These two following coding was to change the name to less complicated
colnames(full_final)[23] = "effect.size" 
colnames(full_final)[24] = "sampling.variance"

# Add this observation level variable which will be called residual
full_final <- full_final %>% mutate(residual = 1:n()) 

meta_model <- rma.mv(effect.size ~ 1, V = sampling.variance, method = "REML",  dfs ="contain", test="t", random = list(~1|Species, ~1|Study, ~1|residual), data = full_final)
# In this rma.va function it was effect size to 1 as a control for the "V = sampling variance". In addition, the method was REML as we wanted to calculate the lnRR meta-analytic model. The random list firstly it was Species as all species could differ and the study and residual was also added as instructed from the task. 

summary(meta_model) # From using the summary function it will present the mulivariate Meta-Analysis Model.
```
>Firstly, sample variance means whether the the study of the samples are listed/ located close to the expected values. Throughout the sampling variance values, it presents that the distribution ranges in around the e-03 between e+03. The extreme was e-12 which is likely to be an outsider as it is extremely far away from the mean absolute deviation. 

### Task 6: Findings and its meanings supported with figures
```{r, table 1}
predict(meta_model) # table 1
# This function was used to calculate the confidence intervals and prediction intervals.
```
>Observing the confidence interval values it suggests that the uncertainty in a sample variable is low. This suggests that the data represents pretty accurate and reliable data of the population. Also, the small ci value demonstrates the likelyhood of getting the similar result. The way pred value is -0.0612 indicates the mean overall across the studies had negative interception. Furthermore, the pi (prediction interval) values presents the heterogeneity in effect size estimation across the studies.

```{r, table 2}
# This is another coding method to calculate/ measure the heterogeneity
orchaRd::i2_ml(meta_model, data = full_final) #table 2
``` 

```{r}
# creating a new rma.mv function for the forest plot
meta_model1 <- rma.mv(effect.size ~ Life.stage, V = sampling.variance, method = "REML",  dfs ="contain", test="t", random = list(~1|Species, ~1|Study, ~1|residual), data = full_final)

summary(meta_model1) 
```
```{r gg-oz-gapminder, Figure 1, fig.cap = "Figure 1. The accumulation response ratio across the different life stages, shows the mean estimates (the circle) are located very close to the vertical dotted line at 0. The thick black line represents the 95% confidence interval and with the longer thin black line that represents the prediction interval. For each stage of the number of samples (k) and the number of studies that the samples were obtained were in brackets. The life stages in larvae are in yellow, Juvenile were in red, adults were in blue and the unknown/ not recorded were in green."}

# Using the previous meta model into this plot
# Making a forest plot using the orchard method 
orchaRd::orchard_plot(meta_model1, mod = "Life.stage", group = "Study", data = full_final, xlab = "Acclimation Response Ratio", angle = 45)
# The thin black line is the prediction interval 
# The thick small black line is the confidence interval
# Mean estimate is the dot circle on the prediction line. 
# K = number samples 
# The numbers in the bracket refers to the number of studies
```
  >According to the first calculation of the prediction intervals, the 95% confidence intervals present the interception. Observing the confidence interval values suggests that the uncertainty in a sample variable is low. This suggests that the data represents pretty accurate and reliable data of the population. (Table 1) As well as the overall meta-analytic mean across the studies was -0.0612 indicating that there is no correlation showing negative interception across the studies. Furthermore, the pi (prediction interval) values present heterogeneity in effect size estimation across the studies. The se value was 0.1228 indicating that the sample mean is accurate and reliable representing the for its population, as the se value is small. The confidence intervals were from -0.3067 to 0.1844 suggesting that the possible range around the estimate to be stable - the population mean. The prediction interval is from -4.2515 to 4.1292 the range a future individual observation will fall, the uncertainty of single observation whether it will be within the distribution of the population. 
  Observing table 2, the second calculation states the heterogeneity of the study. The way the I2_Total states 100% presents that all the variables specifically in the species,studies and residuals do not relate or have any significant correlation to each other thus, has high heterogeneity. The I2_Species was about 4.85% stating that the heterogeneity of the study is due to species, 5.57% due to different studies and 89.57% due to residual.
  The multivariate meta-analysis model (meta model used in task 6) above there were 801 samples involved in this analysis. The test for residual heterogeneity presents that the p value < 0.001 was low indicating that it was significant that the studies were not equal and that the diversity between the variables was detected. The p value for the moderators was 0.3758 suggests that the interactions between the moderating variables.    Instead of the forest plot the orchard plot (Figure 1) was used as it gives similar results (confirmed okay by the lecturer). This plot presents that the variables between the life stages across the fishes had no significant interaction in impacting the effect size. This is because the larvae, juvenile and adults were located approximately very similar and close to 0 presents that the stages does not matter. In addition, the not provided was included as although we do not know what stage but it shows that across the stages the measurements and values were the same. 

### Task 7: Funnel plot
```{r}
funnel(x = full_final$effect.size, vi = full_final$sampling.variance, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray75"), yaxis = "seinv", digits = 2, las = 1, xlab = "Correlation Coefficient (r)", atransf=tanh, legend = TRUE)
# This coding was learnt in the previous lectures/ workshop 

# delete the outliers since the yaxis are way to stretched out
plot(full_final$sampling.variance, full_final$residuals, ylim = c(0.00012, 4))

fun_a <- full_final[-c(17, 16, 497, 255, 564, 6, 562, 220, 673, 163), ] # This is specifically looking at the data and cutting the outliers. 
```
```{r gg-oz-gapminder, Figure 2, fig.cap = "Figure 2. The publication bias of the effect size to the sampling variance across the studies, shows the vertical line at 0 where the shades indicate the p values of the studies."}
# Coding to present a funnel plot
funnel(x = fun_a$effect.size, vi = fun_a$sampling.variance, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray75"), yaxis = "seinv", digits = 2, las = 1, xlab = "Correlation Coefficient (r)", atransf=tanh, legend = TRUE)
```
  >Funnel plot (Figure 2) of the effect size sampling variance. This presents the publication bias possibility. The studies (the dots) are approximately evenly distributed (symmetrical) across the vertical line and across the graph. The reason why no other shaded colours can be observed could be because of too many samples/ large numbers of samples used to analyse. It is expected that the shades can be viewed if the plot was zoomed in. Therefore, no potential publication bias. 

### Task 8: Time-lag plot
```{r gg-oz-gapminder, Figure 3, fig.cap = "Figure 3. The time-lag plot of the inverse sampling variance across the year, shows the effect size throughout the time. The red linear line is placed along the data of the shaded circles demonstrating the spreadness of the sample studies. "}
colnames(fun_a)[3] = "year.online"
colnames(fun_a)[4] = "year.print"

# cumulative meta-analysis & mean effect size change test etc 
ggplot(fun_a, aes(y = effect.size, x = year.online, size = 1/sqrt(sampling.variance))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Fisher's Z-transformed Correlation Coefficient (Zr)", size = "Precision (1/SE)") +
    theme_classic()


# Time-lag explains `r r2_time[1]*100`% of the variation in Zr
```
  >Figure 3 demonstrates the positive slope of the red line suggesting that the mean effect size throughout the year has been slightly increasing (clear relationship with the year). The slope estimate for sampling variance was not significant explaining that there is no publication bias. Not only, but with about equal spread across the line indicates that there is no expected publication bias as each circle was not skewed to a side. This also presents the effect size of the studies compared across the years. The year (online) was chosen since it was published earlier than the printed year date. The smaller precision indicates greater sampling variance which were detected between -5 and 5 of the Fisher’s Z-transformed correlation coefficient which shows that the sample variation is normally distributed.

### Task 9: Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
# Including sampling variance and year as moderators to account for both!
time_bias_meta <- rma.mv(effect.size ~ year.online, V = sampling.variance,
                    random = list(~1|Species,
                                  ~1|Study,
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = fun_a)

# How much variation does time when results were published explain in Zr?
r2_tb <- orchaRd::r2_ml(time_bias_meta)
r2_tb
```
### Task 10: Formal meta-regression model that includes inverse sampling variance to test for file-drawer biases

```{r}
# Centering the sampling variance and the year (mean) to account 
fun_a <- fun_a %>% mutate(Year_center = year.online - mean(year.online)) %>% mutate(sv_center = sampling.variance - mean(sampling.variance))

file_drawer_meta <- rma.mv(effect.size ~ Year_center + 1/sv_center, V = sampling.variance,
                    random = list(~1|Species,
                                  ~1|Study,
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = fun_a)

# How much variation does time when results were published explain in Zr?
r2_tb <- orchaRd::r2_ml(file_drawer_meta)
r2_tb 
```
### Task 11: Potential publication bias based on meta-regression results.
  >Both of the meta-regression models from task 9 and 10, have given the  similar/ same value as the R2 marginal was 0.022 and R2 conditional 0.11. According to this value, the R2 marginal calculates the variance of the fixed effects with the years as the moderators. Furthermore, the R2 conditional value was also low which indicates that the effect size of the studies across the year with in count to fixed and random effects also shows that there is no expected publication bias. Thus, coming to the conclusion that the data analysis of the different reef fish species having no strong evidence that increased acidification had effects on behaviour this result seems to be accurate as according to this meta-analysis it presents with no/ extremely low chance of publication bias. So, there are no possible other contribution that could cause such bias as there is none.

### Task 12: Comparation between meta-analysis of the updated and by Clement et al. (2022) in contribution to publication bias
  >In the updated meta-analysis the conclusion was that there was no publication bias while the paper did have publication bias. The difference was because the Clement et al (2020) paper for meta-analyses has used the absolute lnRR values and that the mean effect size was not computed for each study apart from the raw effect size in the section and has shown that throughout the period the mean effect size had decreased. Not only, how the lnRR was in absolute value but in the updated the data used for the weighted mean effect size magnitudes were not all included such as the removal of the outliers of the mean effect size as mentioned in task 7 and the used was presented in figure 2. Thus, there was a positive relationship between effect size and the chosen journal. This could be why there was an increase in the linear line in the update and not in the paper. 
  The reason why the publication bias result was different was because in the updated data we added the clerk data as well as removing the outlier which could be the reason why there was no publication bias in the updated meta-analysis and yes for clement’s analyses.  For these reasons, selecting the studies with favourable effect size results was the causation of the publication bias. Thus, overall having a different result to the updated meta-analysis as combination of strong, weak and no effect type was about an estimated to have effect type ratio when observing the data. Nevertheless, in Clements study the effect sizes were converted into absolute value due to inherent difficulty which could affect the behaviour however, it is important to take it into factor as to study about the fish in the field when calculating the lnRR to be more realistic data. Also, the reason Clements had publication bias is because the absolute effect sizes cause overestimation of the true effect size which is why the decline effect was observed.
  This was due to the possible publication bias causation across the studies that was used in Clement’s paper that could have led to effect size magnitude declining. The effect of ocean acidification on fish behaviours that declined the effect size are possible due to biological, publication practices and methodological. The greatest factor that had caused the exaggeration in the effect size values was because across the journals the selected samples with high impact factors (causing stronger weaker effect sizes) were cited a lot more which can be observed in Figure 4 in the paper -from favouring/ selective. This results in incorrect and not consistent interpretation of the study of ocean acidification. However, in the update the difference was that the effect type that was used there is a nice mix and use of different impacted studies. 
  Moreover, this can be observed as the data that was collected and used in Clement was significantly skewed is likely due to expermenter and the observation bias. As in the publications/ the references used there was no blinding investigator or observations to measure the measurements which was another possible factor that led to large effect sizes. Furthermore, there was investigator effects causing the decline in effect size in Clement’s analyses is because all the data obtained was from Clements and checked by coauthors for accuracy shows that there is bias as it was not assessed by different assets whether the data would be appropriate to be used for meta-analyses. This allows favouring specific data for the analyses while in the updated all the datas apart from the extreme outliers were taken out. Also there could be methodological bias where the studies or the method of the design and approach could be differently analysed which could result in achieving a decreasing value of effects.

# ii) Reproducibility
(My GitHub Repository)[https://github.com/Han3207/Meta-analysis_OA]

