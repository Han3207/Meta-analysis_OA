---
title: "Assignment #2: Meta-analysis of Ocean Acidification Effects on Behaviour"
output: html_document
date: "2022-10-20"
---

## UNI ID: u7141409
Writing of findings is done using inline code chunks with reference to specific object values. 

Figure and tables have clear and well labelled captions that are informative and correctly referenced within the document 



use meta-analysis to: - estimate the overall effect of ocean acidification on behaviour and determine if these effects are general across studies conducting similar experiments; - understand how variable the effect size is within the literature - what factors (biological, methodological, publication practices) explain variation in effect size.

# i) Statistical Analysis and Interpretation
### Downloading packages
```{r}
library(pacman)
devtools::install_github("daniel1noble/orchaRd", force = TRUE) # Install the orchaRd package
pacman::p_load(readxl, tidyverse, dplyr, Rcpp, ggforce, writexl, flextable, metafor, readr, orchaRd)
```

### 1. Correct analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv) to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.

```{r}
# Importing the data as well as signing it to a variable 
getwd() # to check pathway
OA_data <- read_csv("C:/ANU study/Year 3 2022/BIOL3207/Assignment2/data/Meta-analysis_OA/OA_activitydat_20190302_BIOL3207.csv")

# Cleaning up the data 
glimpse(OA_data)
list(unique(OA_data$species)) # To check spelling errors in species 
list(unique(OA_data$treatment)) # To check spelling errors in treatment

# Omitting NA in the data
OA <- (na.omit(OA_data))

# The n for each species 
table(OA$species)
# The n for treatments 
table(OA$treatment)

# Creating the necessary summary statistics and using flextable() function to tidy the table
OA_sum <- (OA %>%
  group_by(species, treatment) %>% 
  summarise(mean = mean(activity, na.rm = TRUE), sd = sd(activity, na.rm = TRUE),
  n = length(unique(animal_id))) %>% rename(Species = "species"))

OA_table <- flextable(OA_sum)
```

### 2. Through coding, merge the summary statistics generated from 1) with the metadata (i.e., clark_paper_data.csv) from Clark et al. (2020).

```{r}
getwd()
clark <- read_csv("C:/ANU study/Year 3 2022/BIOL3207/Assignment2/data/Meta-analysis_OA/clark_paper_data.csv") # This file contains the metadata

metadata1 <- cbind(clark, OA_sum) # this function is used to get two data frames and then placed in along together
metadata1

# Making a final metadata  for each treatment as it was instructed in task 1. 
final_meta <- pivot_wider(metadata1, names_from = treatment, names_glue = "{treatment}_{.value}", values_from = c("mean", "sd", "n"))  ### Look for a different method.. 

final_meta
```

### 3. Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset (i.e., ocean_meta_data.csv).

```{r}
getwd()
ocean_meta <- read_csv("C:/ANU study/Year 3 2022/BIOL3207/Assignment2/data/Meta-analysis_OA/ocean_meta_data.csv")

dim(ocean_meta)
dim(final_meta)

## Do some renaming of colnames so they match meta-Data_full
meta <- final_meta %>% rename("oa.mean" = CO2_mean,
                            "oa.sd" = CO2_sd,
                            "oa.n" = CO2_n,
                            "ctrl.mean" = control_mean,
                            "ctrl.sd" = control_sd,
                            "ctrl.n" = control_n)

# Reorder col names based on names in meta_data_full
meta <- meta[names(ocean_meta)]

# Check columns are in same order
colnames(ocean_meta) == colnames(meta)

# Check columns are in same order
colnames(ocean_meta) == colnames(meta)

# Bind the two dataframes
full_final <- rbind(ocean_meta, meta)

```

## Meta-analysis
### 4. Correctly calculate log response ratio (lnRR) effect size for every row of the dataframe using metafor's escalc() function.

```{r, Zrcalc}
full_final[1,] # to check how a row looks like

full_final <- metafor::escalc(n1i = oa.n, n2i = ctrl.n, m1i = ctrl.mean, m2i = oa.mean, sd1i = oa.sd, sd2i = ctrl.sd, data = full_final, measure = "ROM")
# yi: vector to specify the observed effect sizes or outcomes.
# vi: vector to specify the corresponding sampling variances.


# calculating the effect size by one hand of one row to check if i got the correct calculation
13.2730^2 #176.1725
72.7890^2 #5298.239
2.9931^2 #8.958648
94.1290^2 #8860.269

(176.1725/ (5298.239*46)) + (8.958648/(8860.269*26)) #0.0007617395

# Seems like i did it wrong so retrying the escalc() function calculation 
full_final <- escalc(n1i = ctrl.n, n2i = oa.n, m1i = ctrl.mean, m2i = oa.mean, sd1i = ctrl.sd, sd2i = oa.sd, data = full_final, measure = "ROM")
# Found that I had to keep the number, mean and sd of the ctrl in the same '2' and oa in '1'

write_xlsx(full_final, "meta-data_ocean_meta.csv")
# The meaning of all the columns can be found in the meta-data_ocean_meta.csv file. You only need the meta-data_ocean_meta.csv file to understand what the columns mean, otherwise, you can ignore it.
```

### 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor's rma.mv() function.

```{r}
colnames(full_final)[23] = "effect.size"
colnames(full_final)[24] = "sampling.variance"

full_final <- full_final %>% mutate(residual = 1:n()) # Add this observation level variable.

?rma.mv
meta_model <- rma.mv(effect.size ~ 1, V = sampling.variance, method = "REML",  dfs ="contain", test="t", random = list(~1|Species, ~1|Study, ~1|residual), data = full_final)

summary(meta_model)

write_xlsx(full_final, "meta-data_ocean_meta.csv")
# The meaning of all the columns can be found in the meta-data_ocean_meta.csv file. You only need the meta-data_ocean_meta.csv file to understand what the columns mean, otherwise, you can ignore it.
```
Firstly, sample variance means whether the the study of the samples are listed/ located close to the expected values. Throughout the sampling variance values, it presents that the distribution ranges in around the e-03 between e+03. The extreme was e-12 which it is likely to be an outsider as it is extremely far away from the mean absolute deviation. 

### 6. Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include:
- talk about the ci's 

```{r}
# Calculate the prediction intervals - for the measures of heterogeneity in effect size estimates across studies
predict(meta_model)
# pred - interception also the mean overall across the studies

orchaRd::i2_ml(meta_model, data = full_final)
# 4.85% states that the heterogeneity of the  study is due to species
# 5.57% states that the heterogeneity of the study is due to different studies
# 89.57% states that the heterogeneity of the study is due to residual
#100% states that the heterogeneity is due to because they all do not relate/ correlate to each other. 
```
### Orchard plot (pretty much the same as forest plot) (part of task 6)
```{r pis, echo=TRUE}
?orchard_plot

# creating a new rma.mv function for the forest plot
meta_model1 <- rma.mv(effect.size ~ Life.stage, V = sampling.variance, method = "REML",  dfs ="contain", test="t", random = list(~1|Species, ~1|Study, ~1|residual), data = full_final)

summary(meta_model1)

# Making a forest plot using the orchard method 
orchaRd::orchard_plot(meta_model1, mod = "Life.stage", group = "Study", data = full_final, xlab = "Acclimation Response Ratio for different life stages", angle = 45)
# The thin black line is the prediction interval 
# The thick small black line is the confidence interval
# Mean estimate is the dot circle on the prediction line. 
# K = number samples 
# The numbers in the bracket refers to the number of studies

# Axes are labelled, number of samples and studies plotted on figure


## - Clearly labelled axes 

```
```{r, Task 6, echo=TRUE, eval=TRUE, tab.cap = "Overall meta-analysis mean and measure of uncertainty around the mean estimate"}



```
The predict() and orchaRd::i2_ml() functions were used to measure the heterogeneity in effect size estimation across the studies. According to the first calculation of the prediction intervals the 95% confidence intervals present that the interception as well as the overall meta-analytic mean across the studies was -0.0612 indicating that there is no correlation. Furthermore, the second calculation states the heterogeneity of the study. The way the I2_Total states 100% presents that all the variables specifically in the species,studies and residuals do not relate or have any significant correlation to each other thus, has high heterogeneity. The I2_Species was about 4.85% stating that the heterogeneity of the study is due to species, 5.57% due to different studies and 89.57% due to residual. 

Instead of the forest plot (Figure 1) the orchard plot was used as it gives similar results (confirmed okay by the lecturer). In this figure 1, the mean estimate (the dot circle that is on the prediction line) across the life stages of the fishes shown to be around very close to the vertical dotted line. In addition looking at the 95% confidence interval the (thick black line) suggests that all of these life stages seem to show… The prediction interval (thin black line) suggests that… For each stage the number of samples are shown as k and the number of studies plotted on figure is in brackets. 

### 7. Funnel plot
```{r , echo=FALSE}
?funnel
par(oma=c(4,4,4,4))

funnel(x = full_final$effect.size, vi = full_final$sampling.variance, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray75"), yaxis = "seinv", digits = 2, las = 1, xlab = "Correlation Coefficient (r)", atransf=tanh, legend = TRUE)

# delete the outliers since the yaxis are way to stretched out
plot(full_final$sampling.variance, full_final$residuals, ylim = c(0.00012, 4))

fun_a <- full_final[-c(17, 16, 497, 255, 564, 6, 562, 220, 673, 163), ]
```

```{r}
# Coding to present a funnel plot
funnel(x = fun_a$effect.size, vi = fun_a$sampling.variance, level = c(0.1, 0.05, 0.01), shade = c("white", "gray55", "gray75"), yaxis = "seinv", digits = 2, las = 1, xlab = "Correlation Coefficient (r)", atransf=tanh, legend = TRUE)

```
Figure 2: Funnel plot of the effect size sampling variance. This presents the publication bias possibility. Observing the graph, it shows that the vertical linear line is placed at 0. The shades are not seen probably because there are still quite large numbers of samples and so the shades are not seen. the shades can be seen if zoomed in. It is to visualize the correlation between the variables. The way how there is no asymmetrical distribution indicates that there is no potential publication bias. 

### 8. Time-lag plot
```{r}
colnames(fun_a)[3] = "year.online"
colnames(fun_a)[4] = "year.print"

# cumulative meta-analysis & mean effect size change test etc 
ggplot(fun_a, aes(y = effect.size, x = year.online, size = 1/sqrt(sampling.variance))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "Fisher's Z-transformed Correlation Coefficient (Zr)", size = "Precision (1/SE)") +
    theme_classic()


# Time-lag explains `r r2_time[1]*100`% of the variation in Zr
```
Figure 3: The time-lag plot presents the effect size changes throughout the time. This presents a linear line which was placed along the data where the shaded areas demonstrate the spreadness of the individuals. This provides information that the linear line is a positive slope which indicates the mean effect size increases when the sampling variance is also large. The way there is about equal distribution across the line shows that the expected publication bias as both directions was shown approximately the same. The time lag plot assesses how effect sizes may or not have changed through time. Looking at this publication bias is so that the average effect size changes on the true mean by accumulating more studies. To see if it has exaggerated the effect size compared with the studies that are done in later years. 

Plot of effect size as a function of publication year (online) - this was chosen as the online year was published first than the printed. Points are scaled in relation to their precision inverse sampling variance. Small points indicate effects with low precision or high sampling variance. (That is how it was shown in the html in week 10.) From this plot the key features about this time-lag bias was that it presents a clear positive relationship with year. Also there are earlier year studies that have much higher sampling variance (i.e., low precision), just like we might expect. In addition these early studies appear to have a far higher (Exaggerated) effect size compared with studies that are done in later years. 

There does appear to be a clear negative relationship with year.
Also of note are that the earlier year studies have much higher sampling variance (i.e., lower precision), just like we might expect.
These early studies appear to have a far higher (exaggerated) effect size compared with studies that are done in later years.


### 9. Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
# Including sampling variance and year as moderators to account for both!
time_bias_meta <- rma.mv(effect.size ~ year.online, V = sampling.variance,
                    random = list(~1|Species,
                                  ~1|Study,
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = fun_a)

summary(time_bias_meta)
# How much variation does time when results were published explain in Zr?
r2_tb <- orchaRd::r2_ml(time_bias_meta)
r2_tb
```
### 10. Formal meta-regression model that includes inverse sampling variance to test for file-drawer biases

```{r}
# Centering the sampling variance and the year (mean) to account 
fun_a <- fun_a %>% mutate(Year_center = year.online - mean(year.online)) %>% mutate(sv_center = sampling.variance - mean(sampling.variance))

file_drawer_meta <- rma.mv(effect.size ~ Year_center + 1/sv_center, V = sampling.variance,
                    random = list(~1|Species,
                                  ~1|Study,
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = fun_a)
summary(file_drawer_meta)

# How much variation does time when results were published explain in Zr?
r2_tb <- orchaRd::r2_ml(file_drawer_meta)
r2_tb 
```

### 11. A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?
This model (task 9) demonstrates from the variation model it will show the percentage of the effect size. The marginal value was about 0.022 which indicates that the variation of the fixed effects/ moderators explains the model very low. Also, the conditional value was around 0.11 suggesting that they account for both the fixed and random effects of year in effect size. This meta-regression model includes a year as a moderator in the test for time-lag bias shows that there was no expected publication bias.

This meta-regression model  (task 10) including the inverse sampling variance the overall mean correlation value was found. This is because the marginal was 0.02 and the conditional was 0.11 this once again presents a similar result as discussed in the previous task.Suggesting that not of the species and study and residual had no correlation between them. Also it demonstrates how there is no publication bias. The mean effect size decrease indicates that there were more studies and so there is low chance of publication biases. This presents with the evidence for no publication bias as the slope estimate for sampling variance was not significant. For instance there was uncertainty around the estimate with a low set of studies that did not result in the expected correlation to be an average 0.

Thus, coming to conclusion that the data analysis of the different reef fish species having no strong evidence that increased acidification had effects on behaviour this result seems to be accurate as according to this meta-analysis it presents with no/ low publication bias. 

### 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?
In the publication bias meta-analysis in the paper 
The concerns about these studies was that... 
- 

# ii) Reproducibility
(My GitHub Repository)[https://github.com/Han3207/Meta-analysis_OA.git]
